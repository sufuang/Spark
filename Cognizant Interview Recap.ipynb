{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognizant Interview Recap\n",
    "  - union() vs unionall( )\n",
    "  - StorageLevel - class\n",
    "  - rank & dense_rank \n",
    "  - Multiple conditions in when clause \n",
    "  - reduce vs  reduceByKey \n",
    "  - repartition vs coalesce \n",
    "  - Copy partion table to another partition table\n",
    "  - Decide the total no. of buckets for a hive table\n",
    "  \n",
    " ## Update  History\n",
    "   - Created by Sophia Yue \n",
    "   - Date:  Feb 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataframe  union() vs unionall()\n",
    " - union() – merge two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n",
    " - unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n",
    " - SQL languages, Union eliminates the duplicates but UnionAll merges two datasets including duplicate records. \n",
    " - in PySpark both behave the same and recommend using DataFrame duplicate() function to remove duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StorageLevel\n",
    "    - class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)     \n",
    "    - RDD storage levels:\n",
    "       -DISK_ONLY = StorageLevel(True, False, False, False, 1)       -\n",
    "       -DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)       -\n",
    "       -MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)       -\n",
    "       -MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)       -\n",
    "       -MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)       -\n",
    "       -MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)      -\n",
    "       -MEMORY_ONLY = StorageLevel(False, True, False, False, 1)       -\n",
    "       -MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)       -\n",
    "       -MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)       -\n",
    "       -MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)       -\n",
    "       -OFF_HEAP = StorageLevel(True, True, True, False, 1)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank & dense_rank\n",
    "    - dense_rnak 'salary' and get employee woth top 3 rank  of salary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "#windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "winSpec  = Window.orderBy(\"salary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|        James|     Sales|  3000|   2|\n",
      "|        Maria|   Finance|  3000|   2|\n",
      "|        James|     Sales|  3000|   2|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        Scott|   Finance|  3300|   6|\n",
      "|          Jen|   Finance|  3900|   7|\n",
      "|       Robert|     Sales|  4100|   8|\n",
      "|         Saif|     Sales|  4100|   8|\n",
      "|      Michael|     Sales|  4600|  10|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rank\"\"\"\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(winSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|        Maria|   Finance|  3000|         2|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        Scott|   Finance|  3300|         3|\n",
      "|          Jen|   Finance|  3900|         4|\n",
      "|       Robert|     Sales|  4100|         5|\n",
      "|         Saif|     Sales|  4100|         5|\n",
      "|      Michael|     Sales|  4600|         6|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"dense_rank\"\"\"\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(winSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------+\n",
      "|employee_name|department|salary|salary_rank|\n",
      "+-------------+----------+------+-----------+\n",
      "|        Kumar| Marketing|  2000|          1|\n",
      "|        James|     Sales|  3000|          2|\n",
      "|        Maria|   Finance|  3000|          2|\n",
      "|        James|     Sales|  3000|          2|\n",
      "|         Jeff| Marketing|  3000|          2|\n",
      "|        Scott|   Finance|  3300|          3|\n",
      "+-------------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"dense_rank\"\"\"\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"salary_rank\",dense_rank().over(winSpec)).filter('salary_rank <= 3 ') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple conditions in when clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|employee_name|department|salary|income|\n",
      "+-------------+----------+------+------+\n",
      "|        James|     Sales|  3000|medium|\n",
      "|      Michael|     Sales|  4600|  high|\n",
      "|       Robert|     Sales|  4100|  high|\n",
      "|        Maria|   Finance|  3000|medium|\n",
      "|        James|     Sales|  3000|medium|\n",
      "|        Scott|   Finance|  3300|medium|\n",
      "|          Jen|   Finance|  3900|medium|\n",
      "|         Jeff| Marketing|  3000|medium|\n",
      "|        Kumar| Marketing|  2000|   low|\n",
      "|         Saif|     Sales|  4100|  high|\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "df.withColumn(\"income\",\n",
    "       when(col(\"salary\") < 3000, \"low\")\n",
    "      .when(col(\"salary\") < 4000, \"medium\")\n",
    "      .otherwise(\"high\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce vs  reduceByKey\n",
    " - both are action functions\n",
    " - reduce(f): Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally.\n",
    " - reduceByKey(func, numPartitions=None, partitionFunc=) : Merge the values for each key using an associative and commutative reduce function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to use reduce\n",
    "def f_merge_dfs(*dfs):\n",
    "    \"\"\"\n",
    "      module name : f_merge_dfs\n",
    "      purpose     : Merge PySpark dataframe row-wise\n",
    "      parameter   :\n",
    "        *dfs      : Any number of PySpark dataframe, and separated by ',' \n",
    "      note        : Adapted from https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark\n",
    "      example     : f_merge_dfs(td1, td2, td3, td4) # merge PySpark dataframe td1, td2, td3, and    td4 \n",
    "      written by  : Sophia Yue \n",
    "    \"\"\"    \n",
    "    from   functools   import reduce \n",
    "    from   pyspark.sql import DataFrame\n",
    "    return reduce(DataFrame.union, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to use reduceByKey\n",
    "# The code below is my exercise for spark straming  which use reduceByKey to get count for eavery word\n",
    "# I skip the code \"connexct to Unix via Ubuntu\"  for the input \n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "# Firewalls might block this!\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repartition vs coalesce with PySpark\n",
    "  -  repartition() is used to increase or decrease the RDD, DataFrame partitions\n",
    "  -  coalesce() is used to only decrease the number of partitions in an efficient way. \n",
    "     - This is optimized or improved version of repartition()\n",
    "     - coalesce uses existing partitions to minimize the amount of data that's shuffled.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy partion table to another partition table\n",
    "    - the example below is  from \n",
    "      - https://stackoverflow.com/questions/24211372/loading-data-from-one-hive-table-to-another-with-partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE IF NOT EXISTS reg_logs (\n",
    "id int,\n",
    "region_code int,\n",
    "count int\n",
    ")\n",
    "PARTITIONED BY (utc_date STRING, utc_hour STRING)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/ad_data/raw/reg_logs';\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS reg_logs_org (\n",
    "id int,\n",
    "region_code int,\n",
    "count int\n",
    ")\n",
    "PARTITIONED BY (utc_date STRING)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/ad_data/reg_logs_org';\n",
    "\n",
    "insert overwrite table reg_logs_org PARTITION (utc_date)\n",
    "select id, region_code, sum(count), utc_date\n",
    "from \n",
    "reg_logs\n",
    "group by \n",
    "utc_date, id, region_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide the total no. of buckets for a hive table\n",
    "    - I read the article below. However, I don't get a clear idea. \n",
    "      - https://stackoverflow.com/questions/30730567/how-can-we-decide-the-total-no-of-buckets-for-a-hive-table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
